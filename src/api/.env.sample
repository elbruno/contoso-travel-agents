# Select the LLM provider
# LLM_PROVIDER=azure-openai
# LLM_PROVIDER=github-models
# LLM_PROVIDER=docker-models
# LLM_PROVIDER=foundry-local
# LLM_PROVIDER=ollama-models

# 1/ Use Azure Local Foundry
# You can find a list of available models by running the
# following command in your terminal: `foundry model list`.
LLM_PROVIDER=foundry-local
AZURE_FOUNDRY_LOCAL_MODEL_ALIAS=phi-4-mini-reasoning

# 2/ Azure OpenAI settings
LLM_PROVIDER=azure-openai
AZURE_OPENAI_ENDPOINT=https://PROJECT-NAME.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o
# Uncomment the following line to use a specific Azure OpenAI API key
# This is reuquired if you are running in a container
# AZURE_OPENAI_API_KEY=

# 3/ Github settings
LLM_PROVIDER=github-models
GITHUB_TOKEN=
GITHUB_MODEL=openai/gpt-4o

# 4/ Docker models settings
LLM_PROVIDER=docker-models
# Use the following endpoint if you are running the model runner in Docker
# DOCKER_MODEL_ENDPOINT=http://model-runner.docker.internal/engines/llama.cpp/v1
# Use the following endpoint if you are running the model runner locally (default port is 12434)
DOCKER_MODEL_ENDPOINT=http://localhost:12434/engines/llama.cpp/v1
DOCKER_MODEL=ai/phi4:14B-Q4_0

# 5/ Ollama models settings
LLM_PROVIDER=ollama-models
# Use the following endpoint if you are running Ollama locally (default port is 11434)
OLLAMA_MODEL_ENDPOINT=http://localhost:11434/v1
OLLAMA_MODEL=llama3.1

# Tools settings (check hostnames in docker-compose)
# Update these URLs to match your local or production environment if needed
MCP_CUSTOMER_QUERY_URL=http://tool-customer-query:8080
MCP_DESTINATION_RECOMMENDATION_URL=http://tool-destination-recommendation:5002
MCP_ITINERARY_PLANNING_URL=http://tool-itinerary-planning:5003
MCP_CODE_EVALUATION_URL=http://tool-code-evaluation:5004
MCP_MODEL_INFERENCE_URL=http://tool-model-inference:5005
MCP_WEB_SEARCH_URL=http://tool-web-search:5006
MCP_ECHO_PING_URL=http://tool-echo-ping:5000
MCP_ECHO_PING_ACCESS_TOKEN=123-this-is-a-fake-token-please-use-a-token-provider

# Opentelemetry settings
OTEL_SERVICE_NAME=api
OTEL_EXPORTER_OTLP_ENDPOINT=http://aspire-dashboard:18889
OTEL_EXPORTER_OTLP_HEADERS=x-otlp-header=header-value